{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00f86382",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from joblib import dump, load\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "from imblearn.under_sampling import NearMiss\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import display\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbbc9e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthetic_minority_over_sampling(output, all_columns_train):\n",
    "    print('Original dataset shape {}'.format(Counter(output)))\n",
    "    sm = SMOTE(random_state=20)\n",
    "    train_input_new, train_output_new = sm.fit_resample(all_columns_train, output)\n",
    "    print('New dataset shape {}'.format(Counter(train_output_new)))\n",
    "    return train_input_new, train_output_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9f55749",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_prevalence(y_actual):\n",
    "    # function to calculate if there is class impalance and prints it\n",
    "    #\n",
    "    # input : the output variable\n",
    "    # \n",
    "    print(y_actual.value_counts()/len(y_actual))\n",
    "    print(y_actual.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e74d740a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_columns_train = np.load('all_columns_train.npy')\n",
    "all_columns_test = np.load('all_columns_test.npy')\n",
    "output = pd.read_pickle('train_output.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "200fa20d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47249, 142)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_columns_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c99c3da9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset shape Counter({'NO': 30860, '>30': 12818, '<30': 3571})\n",
      "New dataset shape Counter({'>30': 30860, 'NO': 30860, '<30': 30860})\n"
     ]
    }
   ],
   "source": [
    "train_input_new, train_output_new = synthetic_minority_over_sampling(output, all_columns_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2ef6f9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train,features_valid,labels_train,labels_valid = train_test_split(train_input_new,train_output_new,test_size = 0.2,train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6a6fb8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NO     0.333936\n",
      "<30    0.333189\n",
      ">30    0.332875\n",
      "Name: readmitted, dtype: float64\n",
      "NO     27705\n",
      "<30    27643\n",
      ">30    27617\n",
      "Name: readmitted, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "calc_prevalence(labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "efecfb43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">30    0.335165\n",
      "<30    0.333912\n",
      "NO     0.330923\n",
      "Name: readmitted, dtype: float64\n",
      ">30    6952\n",
      "<30    6926\n",
      "NO     6864\n",
      "Name: readmitted, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "calc_prevalence(labels_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "374a55eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics_and_return_f1_score(model, features, labels):\n",
    "    predictions = model.predict(features)\n",
    "    f1 = f1_score(labels, predictions, average='micro')\n",
    "    print(\"f1 is {0:.5f}\".format(f1))\n",
    "    print(\"Precision is {0:.5f}\".format(precision_score(labels, predictions, average='micro')))\n",
    "    print(\"Recall is {0:.5f}\".format(recall_score(labels, predictions, average='micro')))\n",
    "    y_true = label_binarize(labels, classes=np.unique(labels))\n",
    "    y_predict = label_binarize(predictions, classes=np.unique(labels))\n",
    "    print(\"AUC is {0:.5f}\".format(roc_auc_score(y_true, y_predict,average='micro')))\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "548d3b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 is 0.47981\n",
      "Precision is 0.47981\n",
      "Recall is 0.47981\n",
      "AUC is 0.60986\n",
      "f1 is 0.48040\n",
      "Precision is 0.48040\n",
      "Recall is 0.48040\n",
      "AUC is 0.61030\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr_model = LogisticRegression()\n",
    "lr_model.fit(X=features_train,y =labels_train)\n",
    "print_metrics(lr_model, features_train, labels_train)\n",
    "print_metrics(lr_model, features_valid, labels_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bf569d",
   "metadata": {},
   "source": [
    "## Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "496b949c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 is 1.00000\n",
      "Precision is 1.00000\n",
      "Recall is 1.00000\n",
      "AUC is 1.00000\n",
      "f1 is 0.81200\n",
      "Precision is 0.81200\n",
      "Recall is 0.81200\n",
      "AUC is 0.85900\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_model = RandomForestClassifier()\n",
    "rf_model.fit(features_train,labels_train)\n",
    "print_metrics(rf_model, features_train, labels_train)\n",
    "print_metrics(rf_model, features_valid, labels_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "96351423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_samples_leaf:  3\n",
      "n_estimators:  100\n",
      "f1 is 0.77591\n",
      "Precision is 0.77591\n",
      "Recall is 0.77591\n",
      "AUC is 0.83194\n",
      "=============================\n",
      "min_samples_leaf:  3\n",
      "n_estimators:  200\n",
      "f1 is 0.77857\n",
      "Precision is 0.77857\n",
      "Recall is 0.77857\n",
      "AUC is 0.83392\n",
      "=============================\n",
      "min_samples_leaf:  3\n",
      "n_estimators:  300\n",
      "f1 is 0.78049\n",
      "Precision is 0.78049\n",
      "Recall is 0.78049\n",
      "AUC is 0.83537\n",
      "=============================\n",
      "min_samples_leaf:  4\n",
      "n_estimators:  100\n",
      "f1 is 0.76859\n",
      "Precision is 0.76859\n",
      "Recall is 0.76859\n",
      "AUC is 0.82644\n",
      "=============================\n",
      "min_samples_leaf:  4\n",
      "n_estimators:  200\n",
      "f1 is 0.76772\n",
      "Precision is 0.76772\n",
      "Recall is 0.76772\n",
      "AUC is 0.82579\n",
      "=============================\n",
      "min_samples_leaf:  4\n",
      "n_estimators:  300\n",
      "f1 is 0.77095\n",
      "Precision is 0.77095\n",
      "Recall is 0.77095\n",
      "AUC is 0.82821\n",
      "=============================\n",
      "min_samples_leaf:  5\n",
      "n_estimators:  100\n",
      "f1 is 0.76126\n",
      "Precision is 0.76126\n",
      "Recall is 0.76126\n",
      "AUC is 0.82094\n",
      "=============================\n",
      "min_samples_leaf:  5\n",
      "n_estimators:  200\n",
      "f1 is 0.76159\n",
      "Precision is 0.76159\n",
      "Recall is 0.76159\n",
      "AUC is 0.82120\n",
      "=============================\n",
      "min_samples_leaf:  5\n",
      "n_estimators:  300\n",
      "f1 is 0.76323\n",
      "Precision is 0.76323\n",
      "Recall is 0.76323\n",
      "AUC is 0.82243\n",
      "=============================\n"
     ]
    }
   ],
   "source": [
    "max_f1 = 0\n",
    "\n",
    "for min_samples_leaf in [3, 4, 5]:\n",
    "    for n_estimators in [100, 200, 300]:        \n",
    "        gs_model = RandomForestClassifier(min_samples_leaf = min_samples_leaf, n_estimators = n_estimators)\n",
    "        gs_model.fit(features_train,labels_train)\n",
    "        print('min_samples_leaf: ', min_samples_leaf)\n",
    "        print('n_estimators: ', n_estimators)\n",
    "        f1 = print_metrics_and_return_f1_score(gs_model, features_valid, labels_valid)\n",
    "        if f1 > max_f1:\n",
    "            max_f1 = f1\n",
    "            best_model = gs_model\n",
    "            best_min_samples_leaf = min_samples_leaf\n",
    "            best_n_estimators = n_estimators\n",
    "        print('=============================')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a395044f",
   "metadata": {},
   "source": [
    "## gradient boosting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "35301be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 is 0.67768\n",
      "Precision is 0.67768\n",
      "Recall is 0.67768\n",
      "AUC is 0.75826\n",
      "f1 is 0.66899\n",
      "Precision is 0.66899\n",
      "Recall is 0.66899\n",
      "AUC is 0.75174\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gbc =GradientBoostingClassifier(random_state=42)\n",
    "gbc.fit(features_train,labels_train)\n",
    "print_metrics(gbc, features_train, labels_train)\n",
    "print_metrics(gbc, features_valid, labels_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c52245",
   "metadata": {},
   "source": [
    "## Generating submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3039641d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = rf_model.predict(all_columns_test)\n",
    "test_data = pd.read_csv(\"test.csv\")\n",
    "test_data['readmitted'] = pred\n",
    "test_data[['encounter_id', 'readmitted']].to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f3b4c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
